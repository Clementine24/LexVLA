<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design.">
  <meta name="keywords" content="Vision Language Alignment, Lexical, Sparse representation, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unified Lexical Representation for Interpretable Visual-Language Alignment</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--   <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unified Lexical Representation for Interpretable Visual-Language Alignment</h1>
          <h1 style="font-size:1.5rem; color:#D05A6E;">
              ðŸŽ‰ Accepted by NeurIPS 2024
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                Yifan Li<sup>1</sup>,
            </span>
            <span class="author-block">
                Yikai Wang<sup>1</sup>,
            </span>
            <span class="author-block">
                Yanwei Fu<sup>1</sup>,
            </span>
            <span class="author-block">
                Dongyu Ru<sup>2</sup>,
            </span>
            <span class="author-block">
                Zheng Zhang<sup>2</sup>,
            </span>
            <span class="author-block">
                Tong He<sup>2</sup>
            </span>
        </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Amazon Web Services</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2407.17827"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://recorder-v3.slideslive.com/?share=93878&s=b86b9eb5-1162-4255-9a38-3b852a41a91a"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
<!--                       <i class="fa fa-circle-play"></i> -->
                   <svg xmlns="http://www.w3.org/2000/svg">
                    <path fill="white" d="M464 256A208 208 0 1 0 48 256a208 208 0 1 0 416 0zM0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zM188.3 147.1c7.6-4.2 16.8-4.1 24.3 .5l144 88c7.1 4.4 11.5 12.1 11.5 20.5s-4.4 16.1-11.5 20.5l-144 88c-7.4 4.5-16.7 4.7-24.3 .5s-12.3-12.2-12.3-20.9l0-176c0-8.7 4.7-16.7 12.3-20.9z"/>
                    </svg>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Clementine24/LexVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- A Fake Code Link. -->
<!--               <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code released soon</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <center><img src="./static/images/teaser.png" width="120%"/></center>
        <p class="has-text-centered">
            LexVLA can generate a lexical representation of the input image (the first wordcloud), or to pick some patches of the image for local lexical information representation (the second wordcloud, with the selected patches boxed in red), and to select the most relevant patches of the image given the text content (the rightmost figure, with caption â€™horseâ€™, with the second-to-last figure is the GT mask).
        </p>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual-Language Alignment (VLA) has gained a lot of attention since CLIPâ€™s groundbreaking work. 
            Although CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. 
            On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, 
            is a natural sparse representation and interpretable, providing exact matches for individual words. 
            However, lexical representations is difficult to learn due to no ground-truth supervision and false-discovery issues, 
            and thus requires complex design to train effectively. 
            In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. 
            We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability. 
            To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words. 
            We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on modest multi-modal dataset and avoid intricate training configurations. 
            On crossmodal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M). 
            We conduct extensive experiments to analyze LexVLA. Codes are available at 
            <a href="https://github.com/Clementine24/LexVLA" target="_blank">https://github.com/Clementine24/LexVLA</a>.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <center><img src="./static/images/model architecture.png" width="100%"/></center>
    <p class="has-text-centered">
      We learn a unified lexical representation with distinct codebooks for text and visual modalities. <br>
      We train LexVLA with the standard contrastive objectives along with the proposed overuse penalty to encourage sparsity while preventing meaningless activation.
    </p>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>

        <!-- Performance under different sparsity -->
        <h3 class="title is-4">Performance under different sparsity</h3>
        <div class="content has-text-centered">
          <center><img src="./static/images/sparsity_performance_v3.png" width="120%"/></center>
          <p>
            LexVLA is robust against the sparsity ratio even in a very high ratio (98.27%, <b>only 296 activated tokens</b>).
          </p>
        </div>

        <!-- Lexical visualization -->
        <h3 class="title is-4">Lexical visualization</h3>
        <div class="content has-text-centered">
          <center><img src="./static/images/alignment cases.png" width="120%"/></center>
          <p>
            Visualization of the image lexical representation obtained by LexVLA. Larger word indicates larger lexical value. The first row represents the complete image, and the second row represent local patches (boxed in red). LexVLA learns a well-aligned lexical representation for both image and patches <b>without local supervision</b>.
          </p>
        </div>

        <!-- PatchDis visualization -->
        <h3 class="title is-4">PatchDis visualization</h3>
        <div class="content has-text-centered">
          <center><img src="./static/images/our_detection_result.png" width="120%"/></center>
          <p>
            <b>PatchDis visualization.</b> The same color indicates the same category. LexVLA correctly predicts the corresponding region, even for the small-scale objects, like the bottle in the first image.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2024unified,
      title={Unified Lexical Representation for Interpretable Visual-Language Alignment},
      author={Li, Yifan and Wang, Yikai and Fu, Yanwei and Ru, Dongyu and Zhang, Zheng and He, Tong},
      journal={Advances in Neural Information Processing Systems},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2407.17827">
        <i class="fas fa-file-pdf"></i>
      </a>
<!--       <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Thanks for the template from <a
            href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
